{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "import io\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime, localtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для просмотра страниц магазина/пользователя и сохранения информации об объявлениях\n",
    "# изображения из объявлений сохраняются в папку, содержащую Avito_crawler.ipynb\n",
    "# информация об объявлениях сохраняется в xlsx. файл\n",
    "\n",
    "def avito_crawler(pages):\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    res1=[]\n",
    "    res2=[]\n",
    "    res3=[]\n",
    "    res4=[]\n",
    "    res5=[]\n",
    "    total=10\n",
    "    core_link = (\"https://www.avito.ru/trade\") #адрес магазина\n",
    "    driver.get(core_link)\n",
    "    for n in range(pages):\n",
    "        count = len(driver.find_elements_by_xpath(\"//a[@class='item-description-title-link']\"))\n",
    "        for m in range(count):\n",
    "            main_link = driver.find_elements_by_xpath(\"//a[@class='item-description-title-link']\")[m].get_attribute(\"href\")\n",
    "            driver.get(main_link)\n",
    "            time.sleep(0.5)\n",
    "            name = driver.find_element_by_xpath(\"//meta[@property = 'og:title']\").get_attribute(\"content\")\n",
    "            print ('This is page {} ad №{} {}'.format(n+1,m,name))\n",
    "            print (\"Name extracted...\")\n",
    "            price = driver.find_element_by_xpath(\"//span[@class='js-item-price']\").get_attribute(\"content\")\n",
    "            print (\"Price extracted...\")\n",
    "            html_doc = urllib.request.urlopen(main_link).read()\n",
    "            soup = BeautifulSoup(html_doc)\n",
    "            soupresult = soup.findAll('p')\n",
    "            description=[]\n",
    "            for desc in soupresult:\n",
    "                description.append(desc.get_text())\n",
    "            print (\"Description extracted...\")\n",
    "            try:\n",
    "                driver.find_elements_by_xpath(\"//li[@data-type='image']\")[0]\n",
    "            except:  \n",
    "                image_link = driver.find_element_by_xpath(\"//img[contains(@alt,'фотография')]\").get_attribute(\"src\")\n",
    "                image_name = driver.find_element_by_xpath(\"//img[contains(@alt,'фотография')]\").get_attribute(\"alt\")\n",
    "                image_name = image_name.translate({ord(i): None for i in '\\/?:*\"><|'})\n",
    "                urllib.request.urlretrieve(image_link, \"Page{} ad{} {}.jpg\".format(n+1,m,image_name))\n",
    "            for l in range(total):\n",
    "                    try:\n",
    "                        driver.find_elements_by_xpath(\"//li[@data-type='image']\")[l+1].click()\n",
    "                        image_link = driver.find_element_by_xpath(\"//img[contains(@alt, 'фотография №{}')]\".format(l+1)).get_attribute(\"src\")\n",
    "                        image_name = driver.find_element_by_xpath(\"//img[contains(@alt, 'фотография №{}')]\".format(l+1)).get_attribute(\"alt\")\n",
    "                        image_name = image_name.translate({ord(i): None for i in '\\/?:*\"><|'})\n",
    "                        urllib.request.urlretrieve(image_link, \"Page{} ad{} {}.jpg\".format(n+1,m,image_name))\n",
    "                    except:\n",
    "                        pass\n",
    "            print (\"Images extracted...\")\n",
    "            res1.append(n+1)\n",
    "            res2.append(m)\n",
    "            res3.append(name)\n",
    "            res4.append(price)\n",
    "            res5.append(description)\n",
    "            driver.back()\n",
    "        if n<(pages-1):\n",
    "            driver.find_element_by_xpath(\"//a[@class='pagination-page js-pagination-next']\").click()  \n",
    "        else:\n",
    "            print(\"Reached final page\")\n",
    "    driver.close()\n",
    "    keys = (\"Номер страницы\",\"Номер объявления\",\"Заголовок\",\"Цена\",\"Описание\")\n",
    "    values = (res1,res2,res3,res4,res5)\n",
    "    new_dict = dict(zip(keys, values))\n",
    "    print (\"Creating database...\")\n",
    "    df = pd.DataFrame(data=new_dict)\n",
    "    df['Описание'] = [','.join(map(str, l)) for l in df['Описание']]\n",
    "    print ('Extracting to Excel...')\n",
    "    currenttime = strftime(\"%Y-%m-%d %H.%M.%S\", localtime())\n",
    "    df.to_excel(\"Avito base {}.xlsx\".format(currenttime))\n",
    "    print (\"Task completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [chromedriver 81.0.4044.138 mac64] driver in cache \n",
      "File found in cache by path [/Users/olegkandybko/.wdm/drivers/chromedriver/81.0.4044.138/mac64/chromedriver]\n",
      "Reached final page\n",
      "Creating database...\n",
      "Extracting to Excel...\n",
      "Task completed\n"
     ]
    }
   ],
   "source": [
    "# в качестве аргумента функции передается количество страниц, которое нужно просмотреть\n",
    "avito_crawler(1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
